# Sarcasm Detection: A Comparative Study of BiLSTM, BERT, and DistilBERT

A course case study comparing classical and transformer-based models for sarcasm detection using the [News Headlines Dataset for Sarcasm Detection](https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection).

## Project Overview
This repository contains the code, notebooks, and documentation for a comparative study that evaluates:
- BiLSTM (with GloVe)
- BERT (bert-base-uncased)
- DistilBERT (distilbert-base-uncased)
- helinivan/english-sarcasm-detector (pre-trained)

## Repository structure
See files and folders included: notebooks/, data/, results/, docs/, README.md, requirements.txt.

## Quick start
1. pip install -r requirements.txt
2. Place dataset under data/
3. Run notebooks in order.
